{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# h5py can read hdf5 dataset\n",
    "import h5py\n",
    "\n",
    "# delete bad data files\n",
    "from send2trash import send2trash\n",
    "\n",
    "# fastmri has some k-space undersampling functions we can use\n",
    "# git clone https://github.com/facebookresearch/fastMRI.git\n",
    "# go to the fastmri directory\n",
    "# pip install -e.\n",
    "import fastmri\n",
    "\n",
    "# We will use this functions to generate masks\n",
    "from fastmri.data.subsample import RandomMaskFunc, EquispacedMaskFunc\n",
    "\n",
    "# sigpy is apparently a good MRI viewing tool\n",
    "# pip install sigpy\n",
    "import sigpy as sp\n",
    "import sigpy.plot as pl\n",
    "\n",
    "import numpy as np\n",
    "# !pip install tf-nightly\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/central/home/vliu/bebi205/dataset_objects'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define constants\n",
    "DATASET = 'knee_singlecoil_train'\n",
    "AXES = {\n",
    "        'knee_singlecoil_train' : (1, 2),\n",
    "        'knee_multicoil_train' : (2, 3),\n",
    "       }\n",
    "\n",
    "log = '1'\n",
    "MODEL_NAME = 'model_1'\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), DATASET)\n",
    "mri_paths = glob.glob(os.path.join(data_path, '*01.h5'))\n",
    "log_paths = os.path.join(os.getcwd(), f'logs/{log}.txt')\n",
    "# data_save_path = os.path.join('/central/groups/BEBi_205_Spring_2021/vliu/', f'dataset_objects')\n",
    "data_save_path = os.path.join(os.getcwd(), f'dataset_objects')\n",
    "data_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = os.path.join('/central/groups/BEBi_205_Spring_2021/vliu', DATASET)\n",
    "# mri_paths = glob.glob(os.path.join(data_path, '*1.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block gets Dataset object with imaginary and real separated\n",
    "def _get_kspace_and_reconstruction_rss(filename):\n",
    "    \"\"\"\n",
    "    @params filename: full path to .h5 mri file\n",
    "    @return kspace data of that particular file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(filename, 'r') as hr:\n",
    "            return hr['kspace'][:], hr['reconstruction_rss'][:]\n",
    "    except:\n",
    "        print(f'Error could not open {filename}')\n",
    "\n",
    "def _get_kspace_undersampled(kspace, center_fractions = [0.04], accelerations = [4]):\n",
    "    \"\"\"\n",
    "    @params kspace: from _get_kspace_and_reconstruction_rss(filename)\n",
    "    @params center_fractions: for undersampling, \n",
    "        N*center_fraction columns in center corresponding to low-frequencies\n",
    "    @params accelerations: how much mri acquisition is sped up\n",
    "    @return undersampled k-space\n",
    "    \"\"\"\n",
    "    mask_func = RandomMaskFunc(\n",
    "        center_fractions = center_fractions, \n",
    "        accelerations = accelerations\n",
    "    )\n",
    "    mask = np.array(mask_func(kspace.shape))\n",
    "    return kspace * mask\n",
    "\n",
    "\n",
    "\n",
    "def _get_mri_im_separated(\n",
    "#     kspace, \n",
    "    reconstruction_rss,\n",
    "    kspace_undersampled, \n",
    "    DATASET\n",
    "):\n",
    "    \"\"\"\n",
    "    separates imaginary from real values\n",
    "    # @params kspace: from _get_kspace_and_reconstruction_rss(filename)\n",
    "    @params reconstruction_rss: reconstructed MR image of fully sampled kspace, provided\n",
    "    @params kspace_undersampled: mask-undersampled k-space from _get_kspace_undersampled\n",
    "    @params DATASET: i.e. 'singlecoil_challenge' or 'multicoil_challenge'\n",
    "    @return (undersampled mri image, fully sampled mri image (i.e. label for GAN))\n",
    "    \"\"\"\n",
    "    undersampled_im = sp.ifft(kspace_undersampled, axes=AXES[DATASET])\n",
    "#     fullysampled_im = sp.ifft(kspace, axes=AXES[DATASET])\n",
    "    \n",
    "    #crop to make sure images are all the size\n",
    "    undersampled_crop = sp.resize(\n",
    "        undersampled_im,\n",
    "        [1, 32, 256, 256]\n",
    "#         [1, 30, 320, 320] # [batch size, height, length, width]\n",
    "    )\n",
    "    \n",
    "    undersampled_crop_real = tf.math.real(undersampled_crop)\n",
    "    undersampled_crop_imag = tf.math.imag(undersampled_crop)\n",
    "    \n",
    "    undersampled_crop = np.stack(\n",
    "        (undersampled_crop_real, undersampled_crop_imag),\n",
    "        axis = 4,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    fullysampled_crop = sp.resize(\n",
    "        reconstruction_rss,\n",
    "        [1, 32, 256, 256]\n",
    "#         [1, 30, 320, 320]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        undersampled_crop,\n",
    "        fullysampled_crop,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_datum_from_single_file_separated(filename, DATASET):\n",
    "    \"\"\"\n",
    "    user-facing function for tf Dataset object\n",
    "    @params filename: full path to .h5 mri file\n",
    "    @params DATASET: i.e. 'singlecoil_challenge' or 'multicoil_challenge'\n",
    "    @return (undersampled mri image, fully sampled mri image (i.e. label for GAN))\n",
    "    \"\"\"\n",
    "    kspace, reconstruction_rss = _get_kspace_and_reconstruction_rss(filename)\n",
    "    kspace_undersampled = _get_kspace_undersampled(kspace)\n",
    "    return _get_mri_im_separated(\n",
    "        reconstruction_rss,\n",
    "        kspace_undersampled,\n",
    "        DATASET,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_files_separated(filenames, DATASET):  \n",
    "    \"\"\"\n",
    "    user-facing function for tf Dataset object\n",
    "    @params filenames: list of full paths to .h5 mri files\n",
    "    @params DATASET: i.e. 'singlecoil_train' or 'multicoil_train'\n",
    "    @return ndarray of \n",
    "        (undersampled mri image, fully sampled mri image (i.e. label for GAN))\n",
    "    \"\"\"\n",
    "    undersampled_images = np.ones((1, 32, 256, 256, 2)) #[bn, h, l, w, c]\n",
    "    fullysampled_images = np.ones((1, 32, 256, 256))\n",
    "#     undersampled_images = np.ones((1, 30, 320, 320, 2)) #[bn, h, l, w, c]\n",
    "#     fullysampled_images = np.ones((1, 30, 320, 320))\n",
    "    for mri_path in filenames:\n",
    "        try:\n",
    "            # undersampled_crop has real and imag components\n",
    "            undersampled_crop, fullysampled_crop = get_datum_from_single_file_separated(\n",
    "                mri_path, DATASET\n",
    "            )\n",
    "               \n",
    "\n",
    "            undersampled_images = np.vstack(\n",
    "                (undersampled_images, undersampled_crop)\n",
    "            )\n",
    "            \n",
    "            \n",
    "            fullysampled_images = np.vstack(\n",
    "                (fullysampled_images, fullysampled_crop)\n",
    "            )\n",
    "            \n",
    "#             print (f'undersampled image shape: {undersampled_crop.shape}')           \n",
    "#             print (f'undersampled images running total shape: {undersampled_images.shape}')\n",
    "#             print (f'fully sampled images running total shape {fullysampled_images.shape} \\n\\n')\n",
    "\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print(f'could not open file {mri_path}')\n",
    "#             send2trash(mri_path)\n",
    "            print(f'sent file {mri_path} to trash')\n",
    "    \n",
    "    # reshape with extra one at the end for channel\n",
    "    fullysampled_images = fullysampled_images.reshape(\n",
    "        (-1, 32, 256, 256, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "    return undersampled_images[1:], fullysampled_images[1:]\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sampled_separated, fully_sampled_separated = get_data_from_files_separated(mri_paths, DATASET)\n",
    "ds_separated = tf.data.Dataset.from_tensor_slices((under_sampled_separated, fully_sampled_separated))\n",
    "ds_separated = ds_separated.shuffle(150, seed = 123, reshuffle_each_iteration = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undersampled size (1, 32, 256, 256, 2) fullysampled size (1, 32, 256, 256, 1)\n",
      "undersampled size (1, 32, 256, 256, 2) fullysampled size (1, 32, 256, 256, 1)\n",
      "undersampled size (1, 32, 256, 256, 2) fullysampled size (1, 32, 256, 256, 1)\n",
      "undersampled size (1, 32, 256, 256, 2) fullysampled size (1, 32, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "for undersampled_im, fullysampled_im in ds_separated.take(20):\n",
    "    undersampled_im = tf.reshape(undersampled_im, (-1, 32, 256, 256, 2))\n",
    "    fullysampled_im = tf.reshape(fullysampled_im, (-1, 32, 256, 256, 1))\n",
    "    print(f'undersampled size {undersampled_im.shape} fullysampled size {fullysampled_im.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 1 GPUs\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv3D, Conv3DTranspose\n",
    "from keras.layers import Add, Concatenate\n",
    "from keras.layers import Activation, LeakyReLU\n",
    "from keras.layers import BatchNormalization, Lambda\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "gpus = [d for d in devices if d.name.lower().startswith('/device:gpu')]\n",
    "print (f'using {len(gpus)} GPUs')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accm(y_true, y_pred):\n",
    "    '''\n",
    "    accuracy metric\n",
    "    '''\n",
    "    y_pred = K.clip(y_pred, -1, 1)\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "\n",
    "def mssim(y_true, y_pred):\n",
    "    '''\n",
    "    mean structural similarity index\n",
    "    '''\n",
    "    costs = 1.0 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 2.0))\n",
    "    return costs\n",
    "\n",
    "def wloss(y_true, y_predict):\n",
    "    '''\n",
    "    Wasserstein loss\n",
    "    '''\n",
    "    return -K.mean(y_true * y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(\n",
    "    kernel_initz,\n",
    "    inp_shape, # 3d, with channels last\n",
    "    trainable = True,\n",
    "):\n",
    "    \n",
    "    gamma_init = tf.random_normal_initializer(1., 0.02)\n",
    "    \n",
    "    inp = Input(shape = inp_shape) # 3d\n",
    "    \n",
    "    l0 = Conv3D(\n",
    "        filters = 16, kernel_size = 4, strides = (1, 2, 2), \n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(inp)\n",
    "    l0 = LeakyReLU(alpha = 0.2)(l0)\n",
    "    \n",
    "    \n",
    "    l1 = Conv3D(\n",
    "        filters = 16 * 2, kernel_size = 4, strides = (1, 2, 2), \n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(l0)\n",
    "    l1 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l1)\n",
    "    l1 = LeakyReLU(alpha = 0.2)(l1)\n",
    "    \n",
    "    \n",
    "    l2 = Conv3D(\n",
    "        filters = 16 * 4, kernel_size = 4, strides = (1, 2, 2), \n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(l1)\n",
    "    l2 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l2)\n",
    "    l2 = LeakyReLU(alpha = 0.2)(l2)\n",
    "    \n",
    "    \n",
    "    l3 = Conv3D(\n",
    "        filters = 16 * 8, kernel_size = 4, strides = (2, 2, 2), \n",
    "        padding = 'same', kernel_initializer = kernel_initz, \n",
    "        )(l2)\n",
    "    l3 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l3)\n",
    "    l3 = LeakyReLU(alpha = 0.2)(l3)\n",
    "    \n",
    "    \n",
    "    l4 = Conv3D(\n",
    "        filters = 16 * 16, kernel_size = 4, strides = (2, 2, 2),\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(l3)\n",
    "    l4 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l4)\n",
    "    l4 = LeakyReLU(alpha = 0.2)(l4)\n",
    "    \n",
    "    \n",
    "    l7 = Conv3D(\n",
    "        filters = 16 * 8, kernel_size = 1, strides = (1, 1, 1),\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(l4)\n",
    "    l7 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l7)\n",
    "    l7 = LeakyReLU(alpha = 0.2)(l7)\n",
    "    \n",
    "    \n",
    "    l8 = Conv3D(\n",
    "        filters = 16 * 4, kernel_size = 1, strides = (1, 1, 1), \n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(l7)\n",
    "    l8 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l8)\n",
    "    l8 = LeakyReLU(alpha = 0.2)(l8)\n",
    "    \n",
    "    \n",
    "    l9 = Conv3D(\n",
    "        filters = 16 * 2, kernel_size = 3, strides = (1, 1, 1),\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(l8)\n",
    "    l9 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l9)\n",
    "    l9 = LeakyReLU(alpha = 0.2)(l9)\n",
    "    \n",
    "    \n",
    "    l10 = Conv3D(\n",
    "        filters = 16 * 8, kernel_size = 3, strides = (1, 1, 1),\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(l9)\n",
    "    l10 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l10)\n",
    "    l10 = LeakyReLU(alpha = 0.2)(l10)\n",
    "    \n",
    "    \n",
    "    l11 = Add()([l7,l10])\n",
    "    l11 = LeakyReLU(alpha = 0.2)(l11)\n",
    "    \n",
    "    \n",
    "    out = Conv3D(\n",
    "        filters = 1, kernel_size = 3, strides = 1,\n",
    "        padding = 'same', kernel_initializer = 'he_normal',\n",
    "        )(l11)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resden(\n",
    "    x, fil_lay, fil_end, beta, \n",
    "    gamma_init, kernel_initz, trainable,\n",
    "):   \n",
    "    \n",
    "    x1 = Conv3D(\n",
    "        filters = fil_lay, kernel_size = 3, strides = 1,\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(x)\n",
    "    x1 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(x1)\n",
    "    x1 = LeakyReLU(alpha = 0.2)(x1)\n",
    "    x1=Concatenate(axis=-1)([x, x1])\n",
    "    \n",
    "    \n",
    "    x2 = Conv3D(\n",
    "        filters = fil_lay, kernel_size = 3, strides = 1,\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(x1)\n",
    "    x2 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(x2)\n",
    "    x2 = LeakyReLU(alpha = 0.2)(x2)\n",
    "    x2 = Concatenate(axis = -1)([x1, x2])\n",
    "     \n",
    "        \n",
    "    x3 = Conv3D(\n",
    "        filters = fil_lay, kernel_size = 3, strides = 1,\n",
    "        padding = 'same', kernel_initializer = kernel_initz, \n",
    "        )(x2)\n",
    "    x3 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(x3)\n",
    "    x3 = LeakyReLU(alpha = 0.2)(x3)\n",
    "    x3 = Concatenate(axis = -1)([x2, x3])\n",
    "    \n",
    "    \n",
    "#     x4 = Conv3D(\n",
    "#         filters = fil_lay, kernel_size = 3, strides = 1,\n",
    "#         padding = 'same', kernel_initializer = kernel_initz, \n",
    "#         )(x3)\n",
    "#     x4 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(x4)\n",
    "#     x4 = LeakyReLU(alpha = 0.2)(x4)\n",
    "#     x4 = Concatenate(axis = -1)([x3, x4])\n",
    "    \n",
    "    \n",
    "    x5 = Conv3D(\n",
    "        filters = fil_end, kernel_size = 3, strides = 1,\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(x3)\n",
    "    \n",
    "    x5 = Lambda(lambda x: x * beta)(x5)\n",
    "    \n",
    "    xout = Add()([x5,x])\n",
    "    return xout\n",
    "\n",
    "def resresden(x, fil_lay, fil_end, beta, gamma_init, kernel_initz, trainable):\n",
    "    \n",
    "    x1 = resden(x,  fil_lay, fil_end, beta, gamma_init, kernel_initz, trainable,)\n",
    "#     x2 = resden(x1, fil_lay, fil_end, beta, gamma_init, kernel_initz, trainable,)\n",
    "    x3 = resden(x, fil_lay, fil_end, beta, gamma_init, kernel_initz, trainable,)\n",
    "    \n",
    "    x3 = Lambda(lambda x : x * beta)(x3)\n",
    "    \n",
    "    xout = Add()([x3,x])\n",
    "    return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inp_shape, kernel_initz, trainable = True,):\n",
    "    gamma_init = tf.random_normal_initializer(1., 0.02)\n",
    "    \n",
    "    fil_lay = 32\n",
    "    fil_end = 512\n",
    "    rrd_count = 12\n",
    "    beta = 0.2\n",
    "\n",
    "    \n",
    "    inp_usamp_imag = Input(inp_shape) # (-1, 32, 256, 256, 2)\n",
    "    \n",
    "    \n",
    "    lay_1dn = Conv3D(\n",
    "        filters = 32, kernel_size = 4, strides = (1, 2, 2), # 32, 128, 128\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(inp_usamp_imag)\n",
    "    lay_1dn = LeakyReLU(alpha = 0.2)(lay_1dn)\n",
    "\n",
    "    \n",
    "    lay_2dn = Conv3D(\n",
    "        filters = 64, kernel_size = 4, strides = (1, 2, 2), # 32, 64, 64\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(lay_1dn)\n",
    "    lay_2dn = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_2dn)\n",
    "    lay_2dn = LeakyReLU(alpha = 0.2)(lay_2dn)\n",
    "\n",
    "    \n",
    "    lay_3dn = Conv3D(\n",
    "        filters = 128, kernel_size = 4, strides = (1, 2, 2), # 32, 32, 32\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(lay_2dn)\n",
    "    lay_3dn = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_3dn)\n",
    "    lay_3dn = LeakyReLU(alpha = 0.2)(lay_3dn)\n",
    "\n",
    "    \n",
    "    lay_4dn = Conv3D(\n",
    "        filters = 256, kernel_size = 4, strides = (2, 2, 2), # 16, 16, 16\n",
    "        padding = 'same', kernel_initializer = kernel_initz, \n",
    "        )(lay_3dn)\n",
    "    lay_4dn = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_4dn)\n",
    "    lay_4dn = LeakyReLU(alpha = 0.2)(lay_4dn)  \n",
    "\n",
    "    \n",
    "    lay_5dn = Conv3D(\n",
    "        filters = 256, kernel_size = 4, strides = (2, 2, 2), # 8, 8, 8\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(lay_4dn)\n",
    "    lay_5dn = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_5dn)\n",
    "    lay_5dn = LeakyReLU(alpha = 0.2)(lay_5dn)\n",
    "\n",
    "\n",
    "    c1 = Conv3D(\n",
    "        filters = fil_end, kernel_size = 3, strides = 1,\n",
    "        padding = 'same', kernel_initializer = kernel_initz, \n",
    "        )(lay_5dn)\n",
    "    \n",
    "    xrrd = c1\n",
    "    for _ in range(rrd_count):\n",
    "        xrrd = resresden(xrrd, fil_lay, fil_end, beta, gamma_init, kernel_initz, trainable)\n",
    "\n",
    "    c2 = Conv3D(\n",
    "        filters = fil_end, kernel_size = 3, strides = 1,\n",
    "        padding = 'same', kernel_initializer = kernel_initz, \n",
    "        )(xrrd)\n",
    "    \n",
    "    \n",
    "    lay_5upc = Add()([c1, c2]) # 8, 8, 8\n",
    "\n",
    "    \n",
    "    lay_4up = Conv3DTranspose(\n",
    "        filters = 256, kernel_size = 4, strides = (2, 2, 2), # 16, 16, 16\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(lay_5upc)\n",
    "    lay_4up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_4up)\n",
    "    lay_4up = Activation('relu')(lay_4up) \n",
    "\n",
    "    lay_4upc = Concatenate(axis = -1)([lay_4up,lay_4dn]) \n",
    "\n",
    "    \n",
    "    \n",
    "    lay_3up = Conv3DTranspose(\n",
    "        filters = 128, kernel_size = 4, strides = (2, 2, 2), # 32, 32, 32\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(lay_4upc) \n",
    "    lay_3up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_3up)\n",
    "    lay_3up = Activation('relu')(lay_3up)\n",
    "\n",
    "    lay_3upc = Concatenate(axis = -1)([lay_3up,lay_3dn])\n",
    "\n",
    "    \n",
    "    lay_2up = Conv3DTranspose(\n",
    "        filters = 64, kernel_size = 4, strides = (1, 2, 2), #32, 64, 64\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(lay_3upc)\n",
    "    lay_2up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_2up)\n",
    "    lay_2up = Activation('relu')(lay_2up)\n",
    "\n",
    "    lay_2upc = Concatenate(axis = -1)([lay_2up, lay_2dn])\n",
    "\n",
    "    \n",
    "    lay_1up = Conv3DTranspose(\n",
    "        filters = 32, kernel_size = 4, strides = (1, 2, 2), #32, 128, 128\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(lay_2upc)\n",
    "    lay_1up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_1up)\n",
    "    lay_1up = Activation('relu')(lay_1up) \n",
    "\n",
    "    lay_1upc = Concatenate(axis = -1)([lay_1up,lay_1dn])\n",
    "\n",
    "    lay_256up = Conv3DTranspose(\n",
    "        filters = 32, kernel_size = 4, strides = (1, 2, 2), #32, 256, 256\n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(lay_1upc)\n",
    "    lay_256up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_256up)\n",
    "    lay_256up = Activation('relu')(lay_256up)\n",
    "\n",
    "    out = Conv3D(\n",
    "        filters = 1, kernel_size = 1, strides = (1, 1, 1), activation = 'tanh', \n",
    "        padding = 'same', kernel_initializer = kernel_initz,\n",
    "        )(lay_256up)\n",
    "\n",
    "    model = Model(inputs = inp_usamp_imag, outputs = out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan_model(gen_model, dis_model, inp_shape):\n",
    "        \n",
    "    dis_model.trainable = False\n",
    "    inp = Input(shape = inp_shape)\n",
    "    out_g = gen_model(inp)\n",
    "    out_dis = dis_model(out_g)\n",
    "    model = Model(inputs = inp, outputs = [out_dis, out_g])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    g_model, d_model, gan_model, \n",
    "    dataset, \n",
    "    n_epochs, n_batch, n_critic, \n",
    "    clip_val, n_patch, \n",
    "    f,\n",
    "):\n",
    "    \n",
    "    bat_per_epo = int(np.ceil(tf.data.experimental.cardinality(dataset).numpy() / n_batch))\n",
    "    half_batch = int(np.ceil(n_batch / 2))\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epo):\n",
    "            \n",
    "            # training the discriminator\n",
    "            for k in range(n_critic):\n",
    "                X = np.ones((1, 32, 256, 256, 1)) #[h, l, w, c]\n",
    "                y = np.ones((1, n_patch, n_patch, n_patch, 1))\n",
    "                \n",
    "                \n",
    "                dataset = dataset.shuffle(tf.data.experimental.cardinality(dataset))\n",
    "                for usamp_data, fsamp_data in dataset.take(half_batch):\n",
    "                    usamp_data = tf.reshape(usamp_data, (-1, 32, 256, 256, 2))\n",
    "                    fsamp_data = tf.reshape(fsamp_data, (-1, 32, 256, 256, 1))\n",
    "                    \n",
    "#                     print(f'usamp shape: {usamp_data.shape}, fsamp shape: {fsamp_data.shape}')\n",
    "                    \n",
    "                    X_real = fsamp_data\n",
    "                    X_fake = g_model.predict(usamp_data)\n",
    "                    \n",
    "                    y_real = np.ones((1, n_patch, n_patch, n_patch, 1))\n",
    "                    y_fake = -np.ones((1, n_patch, n_patch, n_patch, 1))\n",
    "\n",
    "                    X, y = np.vstack((X, X_real)), np.vstack((y, y_real))\n",
    "                    X, y = np.vstack((X, X_fake)), np.vstack((y, y_fake))\n",
    "                    \n",
    "                    \n",
    "\n",
    "                X, y = X[1:], y[1:] # take out first np.ones\n",
    "                \n",
    "#                 print(f'X shape: {X.shape}, y shape: {y.shape}')\n",
    "                d_loss, accuracy = d_model.train_on_batch(X, y)\n",
    "\n",
    "                for l in d_model.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -clip_val, clip_val) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "                    \n",
    "            # training the generator\n",
    "            X_usamps = np.ones((1, 32, 256, 256, 2)) #[h, l, w, c]\n",
    "            X_fsamps = np.ones((1, 32, 256, 256, 1))\n",
    "\n",
    "            for X_usamp, X_fsamp in dataset.take(n_batch):\n",
    "                \n",
    "                X_usamp = tf.reshape(X_usamp, (-1, 32, 256, 256, 2))\n",
    "                X_fsamp = tf.reshape(X_fsamp, (-1, 32, 256, 256, 1))\n",
    "\n",
    "                X_usamps = np.vstack((X_usamps, X_usamp))\n",
    "                X_fsamps = np.vstack((X_fsamps, X_fsamp))\n",
    "                \n",
    "            X_usamps, X_fsamps = X_usamps[1:], X_fsamps[1:] # take out first np.ones\n",
    "            y_gan = np.ones((n_batch, n_patch, n_patch, n_patch, 1))\n",
    "            g_loss = gan_model.train_on_batch ([X_usamps], [y_gan, X_fsamps])\n",
    "            \n",
    "            \n",
    "#             f.write(f'>epoch: {i+1}, batch: {(j+1)/bat_per_epo}, discriminator loss: {d_loss}, acc: {accuracy},  wasserstein: {g_loss[1]},  mae: {g_loss[2]},  mssim: {g_loss[3]}, generator_loss: {g_loss[0]}')\n",
    "            \n",
    "            f.write(f'''>epoch: {i+1}, batch: {np.round((j+1)/bat_per_epo, 6)} \\n''')\n",
    "            f.write(f'''    discriminator loss: {np.round(d_loss, 6)}, generator loss: {np.round(g_loss[0], 6)}, acc: {np.round(accuracy, 6)} \\n\\n\\n''')        \n",
    "    \n",
    "\n",
    "            print (f'''>epoch: {i+1}, batch: {(j+1)/bat_per_epo}''')\n",
    "            print(f'''    discriminator loss: {d_loss}, generator loss: {g_loss[0]}, acc: {accuracy} \\n\\n''')    \n",
    "        \n",
    "        filename = f'{MODEL_NAME}_epoch_{i+1}.h5'\n",
    "        g_model.save(filename)\n",
    "        \n",
    "    f.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3,3,3,608,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dc48024ffb38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_shape_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'he_normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mgan_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_gan_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shape_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ec1315161a28>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(inp_shape, kernel_initz, trainable)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mxrrd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrrd_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mxrrd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresresden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrrd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfil_lay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfil_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     c2 = Conv3D(\n",
      "\u001b[0;32m<ipython-input-5-1d4f443ed72c>\u001b[0m in \u001b[0;36mresresden\u001b[0;34m(x, fil_lay, fil_end, beta, gamma_init, kernel_initz, trainable)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfil_lay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfil_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#     x2 = resden(x1, fil_lay, fil_end, beta, gamma_init, kernel_initz, trainable,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfil_lay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfil_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1d4f443ed72c>\u001b[0m in \u001b[0;36mresden\u001b[0;34m(x, fil_lay, fil_end, beta, gamma_init, kernel_initz, trainable)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     x5 = Conv3D(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfil_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_initz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                                 input_list)\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1088\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1091\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2710\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2711\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2712\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    196\u001b[0m                                        self.filters)\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     self.kernel = self.add_weight(\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mcaching_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m     variable = self._add_variable_with_custom_getter(\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0;31m# \"best effort\" to set the initializer with the highest restore UID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_initializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m     new_variable = getter(\n\u001b[0m\u001b[1;32m    806\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    128\u001b[0m   \u001b[0;31m# can remove the V1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0mvariable_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   return tf_variables.VariableV1(\n\u001b[0m\u001b[1;32m    131\u001b[0m       \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maggregation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0maggregation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariableAggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     return previous_getter(\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                         shape=None):\n\u001b[1;32m    198\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2602\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m     \u001b[0mdistribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distribute_strategy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     return resource_variable_ops.ResourceVariable(\n\u001b[0m\u001b[1;32m   2605\u001b[0m         \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1572\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m       self._init_from_args(\n\u001b[0m\u001b[1;32m   1575\u001b[0m           \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1710\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m               \u001b[0minitial_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointInitialValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_initialize_trackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdditional\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \"\"\"\n\u001b[0;32m--> 409\u001b[0;31m     return super(VarianceScaling, self).__call__(\n\u001b[0m\u001b[1;32m    410\u001b[0m         shape, dtype=_get_dtype(dtype), **kwargs)\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[0;31m# constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m       \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m.87962566103423978\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"untruncated_normal\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mtruncated_normal\u001b[0;34m(self, shape, mean, stddev, dtype)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m     return op(\n\u001b[0m\u001b[1;32m   1091\u001b[0m         shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=self.seed)\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mtruncated_normal\u001b[0;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[1;32m    197\u001b[0m         shape_tensor, dtype, seed=seed1, seed2=seed2)\n\u001b[1;32m    198\u001b[0m     \u001b[0mmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstddev_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bebi205/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3,3,3,608,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add]"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# hyperparameters       \n",
    "n_epochs = 1\n",
    "n_batch = 1\n",
    "n_critic = 1\n",
    "clip_val = 0.05\n",
    "in_shape_gen = (32, 256, 256, 2)\n",
    "in_shape_dis = (32, 256, 256, 1)\n",
    "\n",
    "\n",
    "d_model = discriminator(inp_shape = in_shape_dis, kernel_initz = 'he_normal', trainable = True)\n",
    "opt = Adam(lr = 0.0002, beta_1 = 0.5)\n",
    "d_model.compile(loss = wloss, optimizer = opt, metrics = [accm])\n",
    "# d_model.summary()\n",
    "\n",
    "\n",
    "g_model = generator(inp_shape = in_shape_gen, kernel_initz = 'he_normal', trainable = True)\n",
    "\n",
    "gan_model = define_gan_model(g_model, d_model, in_shape_gen)\n",
    "opt1 = Adam(lr = 0.0001, beta_1 = 0.5)\n",
    "gan_model.compile(loss = [wloss, 'mae', mssim], optimizer = opt1, loss_weights = [0.01, 20.0, 1.0])\n",
    "# g_model.summary()\n",
    "\n",
    "\n",
    "# other paramters\n",
    "n_patch = d_model.output_shape[1]\n",
    "f = open(log_paths, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch: 1, batch: 0.1111111111111111\n",
      "    discriminator loss: 0.12766717374324799, generator loss: 8.094420433044434, acc: 0.28515625 \n",
      "\n",
      "\n",
      ">epoch: 1, batch: 0.2222222222222222\n",
      "    discriminator loss: 1.2649508789763786e-05, generator loss: 6.492024898529053, acc: 0.0 \n",
      "\n",
      "\n",
      ">epoch: 1, batch: 0.3333333333333333\n",
      "    discriminator loss: -0.0013698210241273046, generator loss: 5.073728084564209, acc: 0.0 \n",
      "\n",
      "\n",
      ">epoch: 1, batch: 0.4444444444444444\n",
      "    discriminator loss: -0.0038544279523193836, generator loss: 3.7811505794525146, acc: 0.0 \n",
      "\n",
      "\n",
      ">epoch: 1, batch: 0.5555555555555556\n",
      "    discriminator loss: -0.00378691079095006, generator loss: 3.0285913944244385, acc: 0.0 \n",
      "\n",
      "\n",
      ">epoch: 1, batch: 0.6666666666666666\n",
      "    discriminator loss: -0.005338376387953758, generator loss: 3.8502962589263916, acc: 0.0 \n",
      "\n",
      "\n",
      ">epoch: 1, batch: 0.7777777777777778\n",
      "    discriminator loss: -0.010939040221273899, generator loss: 3.350648880004883, acc: 0.0 \n",
      "\n",
      "\n",
      ">epoch: 1, batch: 0.8888888888888888\n",
      "    discriminator loss: -0.01709231734275818, generator loss: 2.5440714359283447, acc: 0.0 \n",
      "\n",
      "\n",
      ">epoch: 1, batch: 1.0\n",
      "    discriminator loss: -0.023541346192359924, generator loss: 2.2818307876586914, acc: 0.0 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    g_model, d_model, gan_model, \n",
    "    ds_separated, \n",
    "    n_epochs, n_batch, n_critic, \n",
    "    clip_val, n_patch, \n",
    "    f\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bebi205)",
   "language": "python",
   "name": "bebi205"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
