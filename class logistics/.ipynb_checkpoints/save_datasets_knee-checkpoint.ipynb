{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# h5py can read hdf5 dataset\n",
    "import h5py\n",
    "\n",
    "# delete bad data files\n",
    "from send2trash import send2trash\n",
    "\n",
    "# fastmri has some k-space undersampling functions we can use\n",
    "# git clone https://github.com/facebookresearch/fastMRI.git\n",
    "# go to the fastmri directory\n",
    "# pip install -e .\n",
    "import fastmri\n",
    "\n",
    "# We will use this functions to generate masks\n",
    "from fastmri.data.subsample import RandomMaskFunc, EquispacedMaskFunc\n",
    "\n",
    "# sigpy is apparently a good MRI viewing tool\n",
    "# pip install sigpy\n",
    "import sigpy as sp\n",
    "import sigpy.plot as pl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "\n",
    "# # master\n",
    "DATASETS = [\n",
    "    'singlecoil_train',\n",
    "    'singlecoil_val',\n",
    "    'singlecoil_test_v2'\n",
    "]\n",
    "\n",
    "# purely for testing / demo-ing master\n",
    "data_save_path = os.path.join('/central/groups/BEBi_205_Spring_2021/vliu/dataset_objects', 'singlecoil_test_v2')\n",
    "\n",
    "AXES = {\n",
    "        'singlecoil_train' : (1, 2),\n",
    "        'singlecoil_val' : (1, 2),\n",
    "        'singlecoil_test_v2' : (1, 2),\n",
    "       }\n",
    "\n",
    "# # single dataset, for debugging / demo purposes. cwd is home directory\n",
    "# DATASET = 'singlecoil_val' #singlecoil_val, singlecoil_test_v2\n",
    "# data_path = os.path.join('/central/groups/BEBi_205_Spring_2021/vliu', DATASET)\n",
    "# mri_paths = glob.glob(os.path.join(data_path, '*99.h5'))\n",
    "# data_save_path = os.path.join('/central/groups/BEBi_205_Spring_2021/vliu/dataset_objects', DATASET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block gets Dataset object with imaginary and real separated\n",
    "def _get_kspace_and_reconstruction_rss(filename, DATASET):\n",
    "    \"\"\"\n",
    "    @params filename: full path to .h5 mri file\n",
    "    @return kspace data of that particular file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(filename, 'r') as hr:\n",
    "            return hr['kspace'][:], hr['reconstruction_rss'][:]\n",
    "    except:\n",
    "        print(f'Error could not open {filename}')\n",
    "\n",
    "def _get_kspace_undersampled(kspace, center_fractions = [0.04], accelerations = [4]):\n",
    "    \"\"\"\n",
    "    @params kspace: from _get_kspace_and_reconstruction_rss(filename, DATASET)\n",
    "    @params center_fractions: for undersampling, \n",
    "        N*center_fraction columns in center corresponding to low-frequencies\n",
    "    @params accelerations: how much mri acquisition is sped up\n",
    "    @return undersampled k-space\n",
    "    \"\"\"\n",
    "    mask_func = RandomMaskFunc(\n",
    "        center_fractions = center_fractions, \n",
    "        accelerations = accelerations\n",
    "    )\n",
    "    mask = np.array(mask_func(kspace.shape))\n",
    "    return kspace * mask\n",
    "\n",
    "\n",
    "\n",
    "def _get_mri_im_separated(\n",
    "    reconstruction_rss,\n",
    "    kspace_undersampled, \n",
    "    DATASET\n",
    "):\n",
    "    \"\"\"\n",
    "    separates imaginary from real values\n",
    "    # @params kspace: from _get_kspace_and_reconstruction_rss(filename)\n",
    "    @params reconstruction_rss: reconstructed MR image of fully sampled kspace, provided\n",
    "    @params kspace_undersampled: mask-undersampled k-space from _get_kspace_undersampled\n",
    "    @params DATASET: i.e. 'singlecoil_challenge' or 'multicoil_challenge'\n",
    "    @return (undersampled mri image, fully sampled mri image (i.e. label for GAN))\n",
    "    \"\"\"\n",
    "    undersampled_im = sp.ifft(kspace_undersampled, axes = AXES[DATASET])\n",
    "    \n",
    "    #crop to make sure images are all the size\n",
    "    undersampled_crop = sp.resize(\n",
    "        undersampled_im,\n",
    "        [1, 32, 256, 256]\n",
    "    )\n",
    "    \n",
    "    undersampled_crop_real = tf.math.real(undersampled_crop)\n",
    "    undersampled_crop_imag = tf.math.imag(undersampled_crop)\n",
    "    \n",
    "    undersampled_crop = np.stack(\n",
    "        (undersampled_crop_real, undersampled_crop_imag),\n",
    "        axis = 4,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    fullysampled_crop = sp.resize(\n",
    "        reconstruction_rss,\n",
    "        [1, 32, 256, 256]\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        undersampled_crop,\n",
    "        fullysampled_crop,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_datum_from_single_file_separated(filename, DATASET):\n",
    "    \"\"\"\n",
    "    user-facing function for tf Dataset object\n",
    "    @params filename: full path to .h5 mri file\n",
    "    @params DATASET: i.e. 'singlecoil_challenge' or 'multicoil_challenge'\n",
    "    @return (undersampled mri image, fully sampled mri image (i.e. label for GAN))\n",
    "    \"\"\"\n",
    "    kspace, reconstruction_rss = _get_kspace_and_reconstruction_rss(filename, DATASET)\n",
    "    kspace_undersampled = _get_kspace_undersampled(kspace)\n",
    "    return _get_mri_im_separated(\n",
    "        reconstruction_rss,\n",
    "        kspace_undersampled,\n",
    "        DATASET,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_files_separated(filenames, DATASET):  \n",
    "    \"\"\"\n",
    "    user-facing function for tf Dataset object\n",
    "    @params filenames: list of full paths to .h5 mri files\n",
    "    @params DATASET: i.e. 'singlecoil_train' or 'multicoil_train'\n",
    "    @return ndarray of \n",
    "        (undersampled mri image, fully sampled mri image (i.e. label for GAN))\n",
    "    \"\"\"\n",
    "    print_marker = 1\n",
    "    \n",
    "    undersampled_images = np.ones((len(filenames), 32, 256, 256, 2)) #[bn, h, l, w, c]\n",
    "    fullysampled_images = np.ones((len(filenames), 32, 256, 256))\n",
    "\n",
    "    for i, mri_path in enumerate(filenames):\n",
    "        try:\n",
    "            # undersampled_crop has real and imag components\n",
    "            undersampled_crop, fullysampled_crop = get_datum_from_single_file_separated(\n",
    "                mri_path, DATASET\n",
    "            )\n",
    "               \n",
    "\n",
    "            undersampled_images[i] = undersampled_crop\n",
    "            \n",
    "            fullysampled_images[i] = fullysampled_crop\n",
    "            \n",
    "            print_marker += 1\n",
    "            if print_marker % 50 == 0:\n",
    "                print(f'undersampled {print_marker} {DATASET} files thus far')\n",
    "\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print(f'could not process file {mri_path}')\n",
    "#             send2trash(mri_path)\n",
    "            print(f'sent file {mri_path} to trash')\n",
    "    \n",
    "    # reshape with extra one at the end for channel\n",
    "    fullysampled_images = fullysampled_images.reshape(\n",
    "        (-1, 32, 256, 256, 1)\n",
    "    )\n",
    "\n",
    "    return undersampled_images[1:], fullysampled_images[1:]\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "def save_data(mri_paths, DATASET, data_save_path):\n",
    "    '''\n",
    "    save MRIs as tf.data.Dataset object,\n",
    "    undersampled and fully sampled MR images together\n",
    "    @params mri_paths: glob directory of ALL MRI paths\n",
    "    @params dataset: name of DATASET (singlecoil_train)\n",
    "    @params data_save_path: where the tf.data.Dataset is saved\n",
    "    \n",
    "    '''\n",
    "    under_sampled_separated, fully_sampled_separated = get_data_from_files_separated(mri_paths, DATASET)\n",
    "\n",
    "    # save Dataset object\n",
    "    if os.path.isfile(f'{data_save_path}.npz'):\n",
    "        print(f'replacing old {data_save_path}.npz with new data')\n",
    "        os.remove(f'{data_save_path}.npz')\n",
    "            \n",
    "\n",
    "    np.savez(data_save_path, under_sampled_separated, fully_sampled_separated)\n",
    "             \n",
    "    print (f'> > successfully saved Dataset object for {DATASET} \\n\\n')\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def master():\n",
    "    '''\n",
    "    main program to execute save_data for all datasets\n",
    "    including train, val, and test\n",
    "    '''\n",
    "    for DATASET in DATASETS:\n",
    "        print (f'> > > processing {DATASET} images')\n",
    "        # define paths\n",
    "        data_path = os.path.join(\n",
    "            '/central/groups/BEBi_205_Spring_2021/vliu', \n",
    "            DATASET\n",
    "        )\n",
    "        mri_paths = glob.glob(os.path.join(data_path, '*.h5'))\n",
    "        data_save_path = os.path.join(\n",
    "            '/central/groups/BEBi_205_Spring_2021/vliu/dataset_objects', \n",
    "            DATASET\n",
    "        )\n",
    "        save_data(mri_paths, DATASET, data_save_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > processing singlecoil_train images\n",
      "undersampled 50 singlecoil_train files thus far\n",
      "undersampled 100 singlecoil_train files thus far\n",
      "undersampled 150 singlecoil_train files thus far\n",
      "undersampled 200 singlecoil_train files thus far\n",
      "undersampled 250 singlecoil_train files thus far\n",
      "undersampled 300 singlecoil_train files thus far\n",
      "undersampled 350 singlecoil_train files thus far\n",
      "undersampled 400 singlecoil_train files thus far\n",
      "undersampled 450 singlecoil_train files thus far\n",
      "undersampled 500 singlecoil_train files thus far\n",
      "undersampled 550 singlecoil_train files thus far\n",
      "undersampled 600 singlecoil_train files thus far\n",
      "undersampled 650 singlecoil_train files thus far\n",
      "undersampled 700 singlecoil_train files thus far\n",
      "undersampled 750 singlecoil_train files thus far\n",
      "undersampled 800 singlecoil_train files thus far\n",
      "undersampled 850 singlecoil_train files thus far\n",
      "undersampled 900 singlecoil_train files thus far\n",
      "replacing old /central/groups/BEBi_205_Spring_2021/vliu/dataset_objects/singlecoil_train.npz with new data\n",
      "> > successfully saved Dataset object for singlecoil_train \n",
      "\n",
      "\n",
      "> > > processing singlecoil_val images\n",
      "undersampled 50 singlecoil_val files thus far\n",
      "undersampled 100 singlecoil_val files thus far\n",
      "undersampled 150 singlecoil_val files thus far\n",
      "replacing old /central/groups/BEBi_205_Spring_2021/vliu/dataset_objects/singlecoil_val.npz with new data\n",
      "> > successfully saved Dataset object for singlecoil_val \n",
      "\n",
      "\n",
      "> > > processing singlecoil_test_v2 images\n",
      "undersampled 50 singlecoil_test_v2 files thus far\n",
      "replacing old /central/groups/BEBi_205_Spring_2021/vliu/dataset_objects/singlecoil_test_v2.npz with new data\n",
      "> > successfully saved Dataset object for singlecoil_test_v2 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "master()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npzfile = np.load(f'{data_save_path}.npz')\n",
    "# under_sampled_separated = npzfile['arr_0']\n",
    "# fully_sampled_separated = npzfile['arr_1']\n",
    "\n",
    "# ds_separated = tf.data.Dataset.from_tensor_slices((under_sampled_separated, fully_sampled_separated))\n",
    "# ds_separated = ds_separated.shuffle(1000, seed = 123, reshuffle_each_iteration = True)\n",
    "\n",
    "# # check Dataset object was created properly\n",
    "# ds_separated = ds_separated.shuffle(1000)\n",
    "# for undersampled_im, fullysampled_im in ds_separated.take(10):\n",
    "#     undersampled_im = tf.reshape(undersampled_im, (-1, 32, 256, 256, 2))\n",
    "#     fullysampled_im = tf.reshape(fullysampled_im, (-1, 32, 256, 256, 1))\n",
    "#     print(f'undersampled size {undersampled_im.shape} fullysampled size {fullysampled_im.shape}')\n",
    "    \n",
    "\n",
    "# for undersampled_im, fullysampled_im in ds_separated.take(1):\n",
    "#     pl.ImagePlot(undersampled_im[:, :, :, 0]) #this is just real numbers. Don't use this in comparison\n",
    "#     pl.ImagePlot(fullysampled_im[:, :, :, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bebi205)",
   "language": "python",
   "name": "bebi205"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
